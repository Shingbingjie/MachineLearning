# Practice Lab3: Decision Trees
This exercise is about implementing a decision tree from scratch and applying it to the task of classifying whether a mushroom is edible or poisonous. The exercise consists of several steps:

Packages: Importing the necessary packages for the exercise.

Packages: Importing the necessary packages for the exercise.

Dataset: Loading and examining the mushroom dataset, which includes features such as cap color, stalk shape, and whether the mushroom is solitary.

Decision Tree Refresher: Reviewing the steps for building a decision tree, including calculating entropy and information gain, and splitting the dataset.

Building the Tree: Using the functions implemented in the previous steps to build a decision tree by recursively splitting the dataset based on the best feature until the maximum depth is reached.

The exercise includes several coding exercises where the we needs to complete the implementation of functions such as “compute_entropy,” “split_dataset,” “compute_information_gain,” and “get_best_split.” These functions are used to calculate entropy, split the dataset, compute information gain, and determine the best feature to split on, respectively.
